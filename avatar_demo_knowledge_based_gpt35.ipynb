{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2545daee-65d2-4360-8297-df889da19889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import IPython.display as ipd\n",
    "import grpc\n",
    "import os, sys, contextlib\n",
    "import riva.client\n",
    "from txtai.embeddings import Embeddings\n",
    "from txtai.pipeline import Extractor\n",
    "from ast import literal_eval\n",
    "from nemollm.api import NemoLLM\n",
    "from riva.client import RecognitionConfig, StreamingRecognitionConfig, AudioEncoding\n",
    "import riva.client.audio_io\n",
    "from riva.client.audio_io import MicrophoneStream\n",
    "from typing import Iterable\n",
    "import riva.client.proto.riva_asr_pb2 as rasr\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "915604af-2ead-48e4-a9f0-6e7254015e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "URI_TTS = os.get_env('URI_TTS')\n",
    "URI_ASR = os.get_env('URI_ASR')\n",
    "AVATAR_INSTANCE_PATH = os.get_env('AVATAR_INSTANCE_PATH')\n",
    "OPENAI_API_KEY = os.get_env('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f320ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your knowledge base files here\n",
    "K_B = ['wara.txt', 'wasp_1.txt', 'wasp_2.txt']\n",
    "K_B = ['docs/' + file for file in K_B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d675b518-51e0-451c-b8d8-1a35f6cc3f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input audio devices:\n",
      "0: Microsoft Sound Mapper - Input\n",
      "1: Remote Audio\n",
      "4: Primary Sound Capture Driver\n",
      "5: Remote Audio\n",
      "9: Remote Audio\n"
     ]
    }
   ],
   "source": [
    "riva.client.audio_io.list_input_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ac9f08-0841-49df-b8cf-33fad79d4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your input device here\n",
    "ASR_INPUT_DEVICE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83516b40-7ddf-485b-b388-b5cb580c9e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio to Speech Module utilizing the Riva SDK\n",
    "config = riva.client.StreamingRecognitionConfig(\n",
    "    config=riva.client.RecognitionConfig(\n",
    "        encoding=riva.client.AudioEncoding.LINEAR_PCM,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=1,\n",
    "        profanity_filter=False,\n",
    "        enable_automatic_punctuation=True,\n",
    "        verbatim_transcripts=True,\n",
    "        sample_rate_hertz=16000\n",
    "    ),\n",
    "    interim_results=False,\n",
    ")\n",
    "\n",
    "\n",
    "class ASRService:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.auth = riva.client.Auth(uri=URI_ASR)\n",
    "        self.service = riva.client.ASRService(self.auth)\n",
    "        self.sample_rate_hz = 16000\n",
    "        self.file_streaming_chunk = 1600\n",
    "        self.transcript = \"\"\n",
    "        self.default_device_info = riva.client.audio_io.get_default_input_device_info()\n",
    "        self.default_device_index = None if self.default_device_info is None else self.default_device_info['index']\n",
    "\n",
    "    def run(self) -> None:\n",
    "        \"\"\"\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        print(\"ASR service running\")\n",
    "        with riva.client.audio_io.MicrophoneStream(\n",
    "                rate=self.sample_rate_hz,\n",
    "                chunk=self.file_streaming_chunk,\n",
    "                device=ASR_INPUT_DEVICE,\n",
    "        ) as audio_chunk_iterator:\n",
    "            #print(\"mic working\")\n",
    "            self.print_response(responses=self.service.streaming_response_generator(\n",
    "                audio_chunks=audio_chunk_iterator,\n",
    "                streaming_config=config))\n",
    "\n",
    "    def print_response(self, responses: Iterable[rasr.StreamingRecognizeResponse]) -> None:\n",
    "        \"\"\"\n",
    "        :param responses: Streaming Response\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.transcript = \"\"\n",
    "        for response in responses:\n",
    "            if not response.results:\n",
    "                continue\n",
    "\n",
    "            for result in response.results:\n",
    "                if not result.alternatives:\n",
    "                    continue\n",
    "                if result.is_final:\n",
    "                    partial_transcript = result.alternatives[0].transcript\n",
    "                    self.transcript += partial_transcript\n",
    "                    # print(self.transcript)\n",
    "                    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b45e6fa-b6e5-484a-bfb8-03826ec40451",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = URI_TTS\n",
    "auth = riva.client.Auth(uri=uri)\n",
    "\n",
    "tts_service = riva.client.SpeechSynthesisService(auth)\n",
    "\n",
    "sample_rate_hz = 44100\n",
    "req = { \n",
    "        \"language_code\"  : \"en-US\",\n",
    "        \"encoding\"       : riva.client.AudioEncoding.LINEAR_PCM ,   # Currently only LINEAR_PCM is supported\n",
    "        \"sample_rate_hz\" : sample_rate_hz,                          # Generate 44.1KHz audio\n",
    "        \"voice_name\"     : \"English-US.Female-1\"                    # The name of the voice to generate\n",
    "}\n",
    "nchannels = 1\n",
    "sampwidth = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e05d86b-29af-4de9-af11-80ce6f3c2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech to Audio2Face module utilizing the gRPC protocal from audio2face_streaming_utils\n",
    "from audio2face_streaming_utils import push_audio_track\n",
    "import riva.client\n",
    "import io\n",
    "from pydub import AudioSegment\n",
    "from scipy.io.wavfile import read\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Audio2FaceService:\n",
    "    def __init__(self, sample_rate=44100):\n",
    "        \"\"\"\n",
    "        :param sample_rate: sample rate\n",
    "        \"\"\"\n",
    "        self.a2f_url = 'localhost:50051'   # Set it to the port of your local host \n",
    "        self.sample_rate = 44100\n",
    "        self.avatar_instance = AVATAR_INSTANCE_PATH # Set it to the name of your Audio2Face Streaming Instance\n",
    "\n",
    "    def tts_to_wav(self, tts_byte, framerate=22050) -> str:\n",
    "        \"\"\"\n",
    "        :param tts_byte: tts data in byte\n",
    "        :param framerate: framerate\n",
    "        :return: wav byte\n",
    "        \"\"\"\n",
    "        seg = AudioSegment.from_raw(io.BytesIO(tts_byte), sample_width=2, frame_rate=22050, channels=1)\n",
    "        wavIO = io.BytesIO()\n",
    "        seg.export(wavIO, format=\"wav\")\n",
    "        rate, wav = read(io.BytesIO(wavIO.getvalue()))\n",
    "        return wav\n",
    "\n",
    "    def wav_to_numpy_float32(self, wav_byte) -> float:\n",
    "        \"\"\"\n",
    "        :param wav_byte: wav byte\n",
    "        :return: float32\n",
    "        \"\"\"\n",
    "        return wav_byte.astype(np.float32, order='C') / 32768.0\n",
    "\n",
    "    def get_tts_numpy_audio(self, audio) -> float:\n",
    "        \"\"\"\n",
    "        :param audio: audio from tts_to_wav\n",
    "        :return: float32 of the audio\n",
    "        \"\"\"\n",
    "        wav_byte = self.tts_to_wav(audio)\n",
    "        return self.wav_to_numpy_float32(wav_byte)\n",
    "\n",
    "    def make_avatar_speaks(self, audio) -> None:\n",
    "        \"\"\"\n",
    "        :param audio: tts audio\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        push_audio_track(self.a2f_url, self.get_tts_numpy_audio(audio), self.sample_rate, self.avatar_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a78ac8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra content reading and chunking \n",
    "# add the files you want to add to the K.B Model in the K_B list, below\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "\n",
    "docs = []\n",
    "\n",
    "for file in K_B:\n",
    "    with open(file,  encoding=\"utf8\") as f:\n",
    "        doc = f.read()\n",
    "        docs.append(doc)\n",
    "\n",
    "texts = text_splitter.create_documents(docs)\n",
    "data = [t.page_content for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1229a98e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Who are the leaders of the arena? ----\n",
      "\n",
      "The\n",
      " leaders\n",
      " of\n",
      " the\n",
      " arena\n",
      " include\n",
      " Joh\n",
      "anna\n",
      " Bj\n",
      "ör\n",
      "kl\n",
      "und\n",
      ",\n",
      " who\n",
      " is\n",
      " the\n",
      " Project\n",
      " Manager\n",
      " from\n",
      " U\n",
      "me\n",
      "å\n",
      " University\n",
      " &\n",
      " Cod\n",
      "em\n",
      "ill\n",
      " AB\n",
      ",\n",
      " Sand\n",
      "or\n",
      " Al\n",
      "bre\n",
      "cht\n",
      ",\n",
      " the\n",
      " Co\n",
      "-project\n",
      " Manager\n",
      " from\n",
      " K\n",
      "AW\n",
      ",\n",
      " and\n",
      " Iv\n",
      "ana\n",
      " von\n",
      " Pro\n",
      "sch\n",
      "w\n",
      "itz\n",
      ",\n",
      " the\n",
      " Community\n",
      " Manager\n",
      " at\n",
      " W\n",
      "ARA\n",
      " Media\n",
      " &\n",
      " Language\n",
      ".\n",
      " Additionally\n",
      ",\n",
      " the\n",
      " Core\n",
      " Team\n",
      " also\n",
      " consists\n",
      " of\n",
      " other\n",
      " important\n",
      " members\n",
      " such\n",
      " as\n",
      " Anast\n",
      "asia\n",
      " Var\n",
      "ava\n",
      ",\n",
      " Kon\n",
      "rad\n",
      " Toll\n",
      "mar\n",
      ",\n",
      " and\n",
      " Gust\n",
      "av\n",
      " E\n",
      "je\n",
      " H\n",
      "enter\n",
      ",\n",
      " who\n",
      " bring\n",
      " their\n",
      " expertise\n",
      " to\n",
      " drive\n",
      " progress\n",
      " in\n",
      " AI\n",
      " for\n",
      " Media\n",
      " and\n",
      " Language\n",
      ".\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "data = [t.page_content for t in texts]\n",
    "\n",
    "def prompt(question):\n",
    "    return f'{question}[|||||]'\n",
    "\n",
    "\n",
    "def api(prompts):\n",
    "    prompt = prompts[0]\n",
    "    context = prompt.split('[|||||]')[1]\n",
    "    # print(context)\n",
    "    question = prompt.split('[|||||]')[0]\n",
    "    system_prompt = f\"\"\"You are a helpful AI assistant that can help users with their queries. Additionally, you can answer questions about the organization of WARA Media and Language (WARA M&L), and Wallenberg AI, Autonomous Systems and Software Program (WASP).\"\"\"\n",
    "    input_prompt = f\"\"\"You are given the following extracted parts of a long document and a question. Provide a conversational answer.\n",
    "If the question is not directly about WARA M&L or WASP, you can answer the question using your own knowledge and information, without referring to the extracted parts.\n",
    "Question: {question}\n",
    "=========\n",
    "{context}\n",
    "=========\n",
    "Answer:\"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo-1106\",\n",
    "      # model = \"gpt-4-1106-preview\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": input_prompt}\n",
    "      ],\n",
    "      stream=True\n",
    "    )\n",
    "\n",
    "    # response = completion.choices[0].message.content.strip()\n",
    "    return [completion]\n",
    "\n",
    "\n",
    "#Create embeddings model with content support\n",
    "embeddings = Embeddings({\"path\": \"sentence-transformers/all-MiniLM-L6-v2\", \"content\": True})\n",
    "#embeddings = Embeddings({\"path\": \"intfloat/e5-base-v2\", \"content\": True})\n",
    "\n",
    "# Create extractor instance, submit prompts to the Hugging Face inference API\n",
    "#https://medium.com/neuml/introducing-txtai-the-all-in-one-embeddings-database-c721f4ff91ad\n",
    "# The extractor pipeline is txtai’s spin on retrieval augmented generation (RAG).\n",
    "#This pipeline extracts knowledge from content by joining a prompt, context data store (which is the data we want to add to the model, for instance about Wara) and generative model together.\n",
    "extractor = Extractor(embeddings, api, minscore=0.2)\n",
    "\n",
    "#Ad-hoc questions\n",
    "question = \"Who are the leaders of the arena?\"\n",
    "\n",
    "print(\"----\", question, \"----\")\n",
    "completion = extractor([(question, question, prompt(question), False)], data)[0][1]\n",
    "for chunk in completion:        \n",
    "    output0 = chunk.choices[0].delta.content\n",
    "    print(output0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0fb5eaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function to filter out half sentences of the response of the LLM\n",
    "def remove_half_seq(text) -> str:\n",
    "    \"\"\"\n",
    "    :param text: a textual string\n",
    "    :return: a textual string similar or shorter than the input\n",
    "    \"\"\"\n",
    "    marks = [\".\", \"?\", \"!\",\":\",\",\",\";\"]\n",
    "    if text.strip()[-1] in marks:\n",
    "        return text\n",
    "    len_1= []\n",
    "    lens = []\n",
    "    for mark in marks:\n",
    "        splitted_text = text.strip().split(mark)\n",
    "        if len(splitted_text)>1:\n",
    "            len_mark= len(splitted_text[-1])\n",
    "            lens.append(len_mark)\n",
    "        elif len(splitted_text) == 1:\n",
    "            len_1.append(1)\n",
    "    if len(lens)>0:\n",
    "        return text.strip()[:(len(text)-min(lens))]\n",
    "    if len(len_1) == len(marks):\n",
    "        return text\n",
    "        \n",
    "    \n",
    "\n",
    "# function for tts modification for pronunciation:\n",
    "def phonetic_modification(text) -> str:\n",
    "    \"\"\"\n",
    "    :param text: a textual string\n",
    "    :return: a textual in which some words are tagged with new phonetics\n",
    "    \"\"\"\n",
    "    token_list = [\"Johanna\",\"Umeå\", \"AI\",\"LLMs\", \"II\", \"VII\", \"VI\", \"åre\" ]\n",
    "    modified_phonetic = [\"ˈjohana\",\"ˈoomijo\", \"ˈeiˈai\", \"ˈellˈellˈemz\", \"sekˈend\",\"ˈseven\", \"ˈsix\", \"\"]\n",
    "    for token, phon in zip(token_list,modified_phonetic) :\n",
    "        customized_phoneme = '<phoneme ph='+'\"'+ phon+ '\"'+'>'+token+'</phoneme>'\n",
    "        text = text.replace(token,customized_phoneme ) \n",
    "    return text\n",
    "\n",
    "def splitting_text(text) -> list:\n",
    "    \"\"\"\n",
    "    :param text: a textual string\n",
    "    :return:  list of <speak> tag added substrings of the input text\n",
    "    \"\"\"\n",
    "    ## splitting text considering .:\n",
    "    output_chunks0 = text.split(\".\")\n",
    "    # splitting text considering \\n:\n",
    "    output_chunks = []\n",
    "    for opt_ch in output_chunks0:\n",
    "        output_chunks.extend(opt_ch.split(\"\\n\"))\n",
    "    # adding <speak> tags to the substrings\n",
    "    output_ch_sk = []\n",
    "    for chunk in output_chunks:\n",
    "        if chunk.strip() ==\"\":\n",
    "            print(\"chunk\", chunk)\n",
    "        else:\n",
    "            output_ch_sk.append(f'<speak>{chunk}</speak>')\n",
    "    return output_ch_sk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "961fe080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask avatar\n",
      "ASR service running\n",
      "Who is Johanna? \n",
      "Done transcribing\n",
      "Sending audio data...\n",
      "SUCCESS\n",
      "Closed channel\n",
      "Ask avatar\n",
      "ASR service running\n",
      "What is Warra M n l. \n",
      "Done transcribing\n",
      "Sending audio data...\n",
      "SUCCESS\n",
      "Closed channel\n",
      "Sending audio data...\n",
      "SUCCESS\n",
      "Closed channel\n",
      "Sending audio data...\n",
      "SUCCESS\n",
      "Closed channel\n",
      "Ask avatar\n",
      "ASR service running\n",
      "and \n",
      "Done transcribing\n",
      "Sending audio data...\n",
      "SUCCESS\n",
      "Closed channel\n",
      "Sending audio data...\n",
      "SUCCESS\n",
      "Closed channel\n",
      "Sending audio data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m audio \u001b[38;5;241m=\u001b[39m tts_service\u001b[38;5;241m.\u001b[39msynthesize(current_output, language_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men-US\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_rate_hz\u001b[38;5;241m=\u001b[39msample_rate_hz,  voice_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish-US.Male-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m audio_bytes \u001b[38;5;241m=\u001b[39m audio\u001b[38;5;241m.\u001b[39maudio\n\u001b[1;32m---> 42\u001b[0m audio2face_service\u001b[38;5;241m.\u001b[39mmake_avatar_speaks(audio_bytes)\n\u001b[0;32m     43\u001b[0m current_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[7], line 53\u001b[0m, in \u001b[0;36mAudio2FaceService.make_avatar_speaks\u001b[1;34m(self, audio)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_avatar_speaks\u001b[39m(\u001b[38;5;28mself\u001b[39m, audio) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    :param audio: tts audio\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    :return: None\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     push_audio_track(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma2f_url, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tts_numpy_audio(audio), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_rate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavatar_instance)\n",
      "File \u001b[1;32m~\\Riva_Demo_github\\Riva_Demo\\audio2face_streaming_utils.py:42\u001b[0m, in \u001b[0;36mpush_audio_track\u001b[1;34m(url, audio_data, samplerate, instance_name)\u001b[0m\n\u001b[0;32m     40\u001b[0m request\u001b[38;5;241m.\u001b[39mblock_until_playback_is_finished \u001b[38;5;241m=\u001b[39m block_until_playback_is_finished\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending audio data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m response \u001b[38;5;241m=\u001b[39m stub\u001b[38;5;241m.\u001b[39mPushAudio(request)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSUCCESS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\grpc\\_channel.py:1158\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1148\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1155\u001b[0m     (\n\u001b[0;32m   1156\u001b[0m         state,\n\u001b[0;32m   1157\u001b[0m         call,\n\u001b[1;32m-> 1158\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[0;32m   1159\u001b[0m         request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[0;32m   1160\u001b[0m     )\n\u001b[0;32m   1161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\grpc\\_channel.py:1142\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1126\u001b[0m state\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method)\n\u001b[0;32m   1127\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[0;32m   1128\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context,\n\u001b[0;32m   1141\u001b[0m )\n\u001b[1;32m-> 1142\u001b[0m event \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m   1143\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[1;32msrc\\python\\grpcio\\grpc\\_cython\\_cygrpc/channel.pyx.pxi:366\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\python\\grpcio\\grpc\\_cython\\_cygrpc/channel.pyx.pxi:187\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\python\\grpcio\\grpc\\_cython\\_cygrpc/channel.pyx.pxi:181\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\python\\grpcio\\grpc\\_cython\\_cygrpc/completion_queue.pyx.pxi:78\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\python\\grpcio\\grpc\\_cython\\_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\python\\grpcio\\grpc\\_cython\\_cygrpc/completion_queue.pyx.pxi:42\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Avatar + ASR block\n",
    "\n",
    "# K.B model PLUS ASR block + tts modification for pronunciation\n",
    "# Saving the ASR transcripts of the prompts\n",
    "# Filtering out half sequences in the LLM responses\n",
    "# resolving the issue of long input for tts block\n",
    "# saving avatars answer (users prompts are in prompts.txt)\n",
    "\n",
    "audio2face_service = Audio2FaceService()\n",
    "asr_service = ASRService()\n",
    "\n",
    "# writing the prompts in a file:\n",
    "file_asr_transcripts = \"./docs/prompts_transcripts.txt\"\n",
    "file_avatar_replies = \"./docs/full_model_avatar_replies.txt\"\n",
    "    \n",
    "with  open(file_asr_transcripts,\"a\")  as f1, open(file_avatar_replies,\"a\")  as f2:\n",
    "    while True:\n",
    "        print(\"Ask avatar\")\n",
    "        \n",
    "        # speech recognition\n",
    "        asr_service.run()\n",
    "        transcript = asr_service.transcript\n",
    "        print(transcript)\n",
    "        print(\"Done transcribing\")\n",
    "        \n",
    "        # inference block\n",
    "        completion = extractor([(transcript,transcript, prompt(transcript), False)], data)[0][1]\n",
    "        current_output = ''\n",
    "        for chunk in completion:\n",
    "            output0 = chunk.choices[0].delta.content\n",
    "            if output0 is None:\n",
    "                break\n",
    "            current_output += output0\n",
    "            if output0 in ['\\n', '.', '?', '!']:\n",
    "                # tts\n",
    "                # modification for pronunciation:\n",
    "                current_output = phonetic_modification(current_output)\n",
    "                current_output = f'<speak>{current_output}</speak>' \n",
    "\n",
    "                audio = tts_service.synthesize(current_output, language_code=\"en-US\", sample_rate_hz=sample_rate_hz,  voice_name=\"English-US.Male-1\")\n",
    "                audio_bytes = audio.audio\n",
    "                audio2face_service.make_avatar_speaks(audio_bytes)\n",
    "                current_output = ''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
