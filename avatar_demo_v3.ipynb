{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2545daee-65d2-4360-8297-df889da19889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import IPython.display as ipd\n",
    "import grpc\n",
    "import os, sys, contextlib\n",
    "import riva.client\n",
    "from txtai.embeddings import Embeddings\n",
    "from txtai.pipeline import Extractor\n",
    "from ast import literal_eval\n",
    "from nemollm.api import NemoLLM\n",
    "from riva.client import RecognitionConfig, StreamingRecognitionConfig, AudioEncoding\n",
    "import riva.client.audio_io\n",
    "from riva.client.audio_io import MicrophoneStream\n",
    "from typing import Iterable\n",
    "import riva.client.proto.riva_asr_pb2 as rasr\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe168b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "URI_TTS = os.get_env('URI_TTS')\n",
    "URI_ASR = os.get_env('URI_ASR')\n",
    "AVATAR_INSTANCE_PATH = os.get_env('AVATAR_INSTANCE_PATH')\n",
    "NGC_ORG_ID = os.get_env('NGC_ORG_ID')\n",
    "NGC_API_KEY = os.get_env('NGC_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930958f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your knowledge base files here\n",
    "K_B = ['wara.txt', 'wasp_1.txt', 'wasp_2.txt']\n",
    "K_B = ['docs/' + file for file in K_B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d675b518-51e0-451c-b8d8-1a35f6cc3f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input audio devices:\n",
      "0: Microsoft Sound Mapper - Input\n",
      "1: Remote Audio\n",
      "4: Primary Sound Capture Driver\n",
      "5: Remote Audio\n",
      "9: Remote Audio\n"
     ]
    }
   ],
   "source": [
    "riva.client.audio_io.list_input_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6ac9f08-0841-49df-b8cf-33fad79d4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your input device here\n",
    "ASR_INPUT_DEVICE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83516b40-7ddf-485b-b388-b5cb580c9e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio to Speech Module utilizing the Riva SDK\n",
    "config = riva.client.StreamingRecognitionConfig(\n",
    "    config=riva.client.RecognitionConfig(\n",
    "        encoding=riva.client.AudioEncoding.LINEAR_PCM,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=1,\n",
    "        profanity_filter=False,\n",
    "        enable_automatic_punctuation=True,\n",
    "        verbatim_transcripts=True,\n",
    "        sample_rate_hertz=16000\n",
    "    ),\n",
    "    interim_results=False,\n",
    ")\n",
    "\n",
    "\n",
    "class ASRService:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.auth = riva.client.Auth(uri=URI_ASR)\n",
    "        self.service = riva.client.ASRService(self.auth)\n",
    "        self.sample_rate_hz = 16000\n",
    "        self.file_streaming_chunk = 1600\n",
    "        self.transcript = \"\"\n",
    "        self.default_device_info = riva.client.audio_io.get_default_input_device_info()\n",
    "        self.default_device_index = None if self.default_device_info is None else self.default_device_info['index']\n",
    "\n",
    "    def run(self) -> None:\n",
    "        \"\"\"\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        print(\"ASR service running\")\n",
    "        with riva.client.audio_io.MicrophoneStream(\n",
    "                rate=self.sample_rate_hz,\n",
    "                chunk=self.file_streaming_chunk,\n",
    "                device=ASR_INPUT_DEVICE,\n",
    "        ) as audio_chunk_iterator:\n",
    "            print(\"mic working\")\n",
    "            self.print_response(responses=self.service.streaming_response_generator(\n",
    "                audio_chunks=audio_chunk_iterator,\n",
    "                streaming_config=config))\n",
    "\n",
    "    def print_response(self, responses: Iterable[rasr.StreamingRecognizeResponse]) -> None:\n",
    "        \"\"\"\n",
    "        :param responses: Streaming Response\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.transcript = \"\"\n",
    "        for response in responses:\n",
    "            if not response.results:\n",
    "                continue\n",
    "\n",
    "            for result in response.results:\n",
    "                if not result.alternatives:\n",
    "                    continue\n",
    "                if result.is_final:\n",
    "                    partial_transcript = result.alternatives[0].transcript\n",
    "                    self.transcript += partial_transcript\n",
    "                    # print(self.transcript)\n",
    "                    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b45e6fa-b6e5-484a-bfb8-03826ec40451",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = URI_TTS\n",
    "auth = riva.client.Auth(uri=uri)\n",
    "\n",
    "tts_service = riva.client.SpeechSynthesisService(auth)\n",
    "\n",
    "sample_rate_hz = 44100\n",
    "req = { \n",
    "        \"language_code\"  : \"en-US\",\n",
    "        \"encoding\"       : riva.client.AudioEncoding.LINEAR_PCM ,   # Currently only LINEAR_PCM is supported\n",
    "        \"sample_rate_hz\" : sample_rate_hz,                          # Generate 44.1KHz audio\n",
    "        \"voice_name\"     : \"English-US.Female-1\"                    # The name of the voice to generate\n",
    "}\n",
    "nchannels = 1\n",
    "sampwidth = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e05d86b-29af-4de9-af11-80ce6f3c2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech to Audio2Face module utilizing the gRPC protocal from audio2face_streaming_utils\n",
    "from audio2face_streaming_utils import push_audio_track\n",
    "import riva.client\n",
    "import io\n",
    "from pydub import AudioSegment\n",
    "from scipy.io.wavfile import read\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Audio2FaceService:\n",
    "    def __init__(self, sample_rate=44100):\n",
    "        \"\"\"\n",
    "        :param sample_rate: sample rate\n",
    "        \"\"\"\n",
    "        self.a2f_url = 'localhost:50051'   # Set it to the port of your local host \n",
    "        self.sample_rate = 44100\n",
    "        self.avatar_instance = AVATAR_INSTANCE_PATH # Set it to the name of your Audio2Face Streaming Instance\n",
    "\n",
    "    def tts_to_wav(self, tts_byte, framerate=22050) -> str:\n",
    "        \"\"\"\n",
    "        :param tts_byte: tts data in byte\n",
    "        :param framerate: framerate\n",
    "        :return: wav byte\n",
    "        \"\"\"\n",
    "        seg = AudioSegment.from_raw(io.BytesIO(tts_byte), sample_width=2, frame_rate=22050, channels=1)\n",
    "        wavIO = io.BytesIO()\n",
    "        seg.export(wavIO, format=\"wav\")\n",
    "        rate, wav = read(io.BytesIO(wavIO.getvalue()))\n",
    "        return wav\n",
    "\n",
    "    def wav_to_numpy_float32(self, wav_byte) -> float:\n",
    "        \"\"\"\n",
    "        :param wav_byte: wav byte\n",
    "        :return: float32\n",
    "        \"\"\"\n",
    "        return wav_byte.astype(np.float32, order='C') / 32768.0\n",
    "\n",
    "    def get_tts_numpy_audio(self, audio) -> float:\n",
    "        \"\"\"\n",
    "        :param audio: audio from tts_to_wav\n",
    "        :return: float32 of the audio\n",
    "        \"\"\"\n",
    "        wav_byte = self.tts_to_wav(audio)\n",
    "        return self.wav_to_numpy_float32(wav_byte)\n",
    "\n",
    "    def make_avatar_speaks(self, audio) -> None:\n",
    "        \"\"\"\n",
    "        :param audio: tts audio\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        push_audio_track(self.a2f_url, self.get_tts_numpy_audio(audio), self.sample_rate, self.avatar_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a78ac8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra content reading and chunking \n",
    "# add the files you want to add to the K.B Model in the K_B list, below\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "\n",
    "docs = []\n",
    "\n",
    "for file in K_B:\n",
    "    with open(file,  encoding=\"utf8\") as f:\n",
    "        doc = f.read()\n",
    "        docs.append(doc)\n",
    "\n",
    "texts = text_splitter.create_documents(docs)\n",
    "data = [t.page_content for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cb81512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completion_labels': [{'class_name': 'nontoxic', 'score': 0.96920747}],\n",
      " 'cumlogprobs': -0.023525752,\n",
      " 'prompt_labels': [{'class_name': 'nontoxic', 'score': 0.9968957}],\n",
      " 'text': ' greenhouse gas concentrations concentrations'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "connection = NemoLLM(api_host=\"https://api.llm.ngc.nvidia.com/v1\", org_id=NGC_ORG_ID, api_key=NGC_API_KEY, )\n",
    "\n",
    "response = connection.generate(\n",
    "  prompt=\"context: The Intergovernmental Panel on Climate Change (IPCC) is a scientific intergovernmental body under the auspices of the United Nations, set up at the request of member governments. It was first established in 1988 by two United Nations organizations, the World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP), and later endorsed by the United Nations General Assembly through Resolution 43/53. Membership of the IPCC is open to all members of the WMO and UNEP. The IPCC produces reports that support the United Nations Framework Convention on Climate Change (UNFCCC), which is the main international treaty on climate change. The ultimate objective of the UNFCCC is to \\\"stabilize greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic [i.e., human-induced] interference with the climate system\\\". IPCC reports cover \\\"the scientific, technical and socio-economic information relevant to understanding the scientific basis of risk of human-induced climate change, its potential impacts and options for adaptation and mitigation.\\\" question: What does the UN want to stabilize?\",\n",
    "  model=\"gpt-43b-002\",\n",
    "  stop=[],\n",
    "  tokens_to_generate=20,\n",
    "  temperature=1.0,\n",
    "  top_k=1,\n",
    "  top_p=0.0,\n",
    "  random_seed=0,\n",
    "  beam_search_diversity_rate=0.0,\n",
    "  beam_width=1,\n",
    "  repetition_penalty=1.0,\n",
    "  length_penalty=1.0,\n",
    ")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1229a98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Where is Åre event? ----\n",
      "CONTEXT:  [' We also arrange different workshops and event with our collaborative partners such as NVIDIA and Google.\\n\\n\\nCollaborate with us\\nWe welcome any party that is willing and able to drive progress in AI for Media and Language. We interact with both established and emerging enterprises, as well as international initiatives such as Mila in Montreal and the British DPP.\\n\\nFollow us on LinkedIn to be the first to know about upcoming events and opportunities. Follow us on LinkedIn to be the first to know about upcoming events and opportunities.\\n\\n\\nThe Core Team\\nJohanna Björklund, Project Manager, Umeå University & Codemill AB\\n\\nSandor Albrecht, Co-project Manager, KAW\\n\\nIvana von Proschwitz, Community Manager, WARA Media & Language\\n\\nAnastasia Varava, Data Scientist, SEB\\n\\nKonrad Tollmar, Research Director, EA Games / Associate Professor, KTH Royal Institute of Technology\\n\\nGustav Eje Henter, Assistant Professor, KTH Royal Institute of Technology Platforms for research and demonstration in collaboration with other parties. The participating universities have substantial existing infrastructures that can be leveraged for this purpose. Further reinforcement will be achieved by including industrial demonstrators, or even by aiming for new larger highly visible initiatives on an international scale.\\nQ: Where is Åre event?\\nA: ']\n",
      "[('Where is Åre event?', ' The event is in Umeå, Sweden..')]\n"
     ]
    }
   ],
   "source": [
    "# A general function that considers the both cases of having and non having extra context together with the input prompt \n",
    "\n",
    "def prompt(question):\n",
    "    return f'Q: {question}[|||||]'\n",
    "\n",
    "def api(prompts):\n",
    "        new_prompts = [prompt.split('[|||||]')[1] + '\\n' + prompt.split('[|||||]')[0] + '\\nA: ' for prompt in prompts]\n",
    "        print(\"CONTEXT: \", new_prompts)\n",
    "        responses = connection.generate_multiple(\n",
    "          prompts=new_prompts,\n",
    "          model=\"gpt-43b-002\",\n",
    "          stop=['\\nQ', '\\nA'],\n",
    "          tokens_to_generate=200,\n",
    "          temperature=0.9,\n",
    "          top_k=1,\n",
    "          top_p=0.0,\n",
    "          random_seed=0,\n",
    "          beam_search_diversity_rate=0.0,\n",
    "          beam_width=1,\n",
    "          repetition_penalty=1.0,\n",
    "          length_penalty=1.0,\n",
    "        )\n",
    "        return [response['text'] for response in responses]\n",
    "\n",
    "\n",
    "#Create embeddings model with content support\n",
    "embeddings = Embeddings({\"path\": \"sentence-transformers/all-MiniLM-L6-v2\", \"content\": True})\n",
    "#embeddings = Embeddings({\"path\": \"intfloat/e5-base-v2\", \"content\": True})\n",
    "\n",
    "# Create extractor instance, submit prompts to the Hugging Face inference API\n",
    "#https://medium.com/neuml/introducing-txtai-the-all-in-one-embeddings-database-c721f4ff91ad\n",
    "# The extractor pipeline is txtai’s spin on retrieval augmented generation (RAG).\n",
    "#This pipeline extracts knowledge from content by joining a prompt, context data store (which is the data we want to add to the model, for instance about Wara) and generative model together.\n",
    "extractor = Extractor(embeddings, api, minscore=0.25)\n",
    "\n",
    "#Ad-hoc questions\n",
    "question = \"Where is Åre event?\"\n",
    "\n",
    "print(\"----\", question, \"----\")\n",
    "print(extractor([(question, question, prompt(question), False)], data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0fb5eaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function to filter out half sentences of the response of the LLM\n",
    "def remove_half_seq(text) -> str:\n",
    "    \"\"\"\n",
    "    :param text: a textual string\n",
    "    :return: a textual string similar or shorter than the input\n",
    "    \"\"\"\n",
    "    marks = [\".\", \"?\", \"!\",\":\",\",\",\";\"]\n",
    "    if text.strip()[-1] in marks:\n",
    "        return text\n",
    "    len_1= []\n",
    "    lens = []\n",
    "    for mark in marks:\n",
    "        splitted_text = text.strip().split(mark)\n",
    "        if len(splitted_text)>1:\n",
    "            len_mark= len(splitted_text[-1])\n",
    "            lens.append(len_mark)\n",
    "        elif len(splitted_text) == 1:\n",
    "            len_1.append(1)\n",
    "    if len(lens)>0:\n",
    "        return text.strip()[:(len(text)-min(lens))]\n",
    "    if len(len_1) == len(marks):\n",
    "        return text\n",
    "        \n",
    "    \n",
    "\n",
    "# function for tts modification for pronunciation:\n",
    "def phonetic_modification(text) -> str:\n",
    "    \"\"\"\n",
    "    :param text: a textual string\n",
    "    :return: a textual in which some words are tagged with new phonetics\n",
    "    \"\"\"\n",
    "    token_list = [\"Johanna\",\"Umeå\", \"AI\",\"LLMs\", \"II\", \"VII\", \"VI\", \"åre\" ]\n",
    "    modified_phonetic = [\"ˈjohana\",\"ˈoomijo\", \"ˈeiˈai\", \"ˈellˈellˈemz\", \"sekˈend\",\"ˈseven\", \"ˈsix\", \"\"]\n",
    "    for token, phon in zip(token_list,modified_phonetic) :\n",
    "        customized_phoneme = '<phoneme ph='+'\"'+ phon+ '\"'+'>'+token+'</phoneme>'\n",
    "        text = text.replace(token,customized_phoneme ) \n",
    "    return text\n",
    "\n",
    "def splitting_text(text) -> list:\n",
    "    \"\"\"\n",
    "    :param text: a textual string\n",
    "    :return:  list of <speak> tag added substrings of the input text\n",
    "    \"\"\"\n",
    "    ## splitting text considering .:\n",
    "    output_chunks0 = text.split(\".\")\n",
    "    # splitting text considering \\n:\n",
    "    output_chunks = []\n",
    "    for opt_ch in output_chunks0:\n",
    "        output_chunks.extend(opt_ch.split(\"\\n\"))\n",
    "    # adding <speak> tags to the substrings\n",
    "    output_ch_sk = []\n",
    "    for chunk in output_chunks:\n",
    "        if chunk.strip() ==\"\":\n",
    "            print(\"chunk\", chunk)\n",
    "        else:\n",
    "            output_ch_sk.append(f'<speak>{chunk}</speak>')\n",
    "    return output_ch_sk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "961fe080",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask avatar\n",
      "ASR service running\n",
      "mic working\n",
      "ASR transcript: Hello. \n",
      "CONTEXT:  [' \\nQ: Hello. \\nA: ']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASR transcript:\u001b[39m\u001b[38;5;124m\"\u001b[39m,transcript)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# inference block\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m text \u001b[38;5;241m=\u001b[39m extractor([(transcript,transcript, prompt(transcript), \u001b[38;5;28;01mFalse\u001b[39;00m)], data)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Detecting removing empty replies from the LLM\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\txtai\\pipeline\\text\\extractor.py:119\u001b[0m, in \u001b[0;36mExtractor.__call__\u001b[1;34m(self, queue, texts)\u001b[0m\n\u001b[0;32m    116\u001b[0m     snippets\u001b[38;5;241m.\u001b[39mappend(snippet)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Run pipeline and return answers\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m answers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manswers(names, questions, contexts, [[text \u001b[38;5;28;01mfor\u001b[39;00m _, text, _ \u001b[38;5;129;01min\u001b[39;00m topn] \u001b[38;5;28;01mfor\u001b[39;00m topn \u001b[38;5;129;01min\u001b[39;00m topns], snippets)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Apply output formatting to answers and return\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(inputs, queries, answers, topns)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\txtai\\pipeline\\text\\extractor.py:305\u001b[0m, in \u001b[0;36mExtractor.answers\u001b[1;34m(self, names, questions, contexts, topns, snippets)\u001b[0m\n\u001b[0;32m    302\u001b[0m     answers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(questions, contexts)\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;66;03m# Combine question and context into single text field for generative pipelines\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m     answers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestions[x]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(contexts)])\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Extract and format answer\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, answer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(answers):\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;66;03m# Resolve snippet if necessary\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m, in \u001b[0;36mapi\u001b[1;34m(prompts)\u001b[0m\n\u001b[0;32m      7\u001b[0m new_prompts \u001b[38;5;241m=\u001b[39m [prompt\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[|||||]\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m prompt\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[|||||]\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mA: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONTEXT: \u001b[39m\u001b[38;5;124m\"\u001b[39m, new_prompts)\n\u001b[1;32m----> 9\u001b[0m responses \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mgenerate_multiple(\n\u001b[0;32m     10\u001b[0m   prompts\u001b[38;5;241m=\u001b[39mnew_prompts,\n\u001b[0;32m     11\u001b[0m   model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-43b-002\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m   stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     13\u001b[0m   tokens_to_generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[0;32m     14\u001b[0m   temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[0;32m     15\u001b[0m   top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     16\u001b[0m   top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     17\u001b[0m   random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     18\u001b[0m   beam_search_diversity_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     19\u001b[0m   beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     20\u001b[0m   repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[0;32m     21\u001b[0m   length_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m responses]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nemollm\\api.py:435\u001b[0m, in \u001b[0;36mNemoLLM.generate_multiple\u001b[1;34m(self, model, prompts, customization_id, return_type, tokens_to_generate, logprobs, temperature, top_p, top_k, stop, random_seed, repetition_penalty, beam_search_diversity_rate, beam_width, length_penalty, disable_logging)\u001b[0m\n\u001b[0;32m    432\u001b[0m     futures\u001b[38;5;241m.\u001b[39mappend(future)\n\u001b[0;32m    434\u001b[0m responses_json \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompts)\n\u001b[1;32m--> 435\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(futures):\n\u001b[0;32m    436\u001b[0m     response \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m    437\u001b[0m     NemoLLM\u001b[38;5;241m.\u001b[39mhandle_response(response)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\concurrent\\futures\\_base.py:243\u001b[0m, in \u001b[0;36mas_completed\u001b[1;34m(fs, timeout)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[0;32m    240\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    241\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[1;32m--> 243\u001b[0m waiter\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mwait(wait_timeout)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n\u001b[0;32m    246\u001b[0m     finished \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39mfinished_futures\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Avatar + ASR block\n",
    "\n",
    "# K.B model PLUS ASR block + tts modification for pronunciation\n",
    "# Saving the ASR transcripts of the prompts\n",
    "# Filtering out half sequences in the LLM responses\n",
    "# resolving the issue of long input for tts block\n",
    "# saving avatars answer (users prompts are in prompts.txt)\n",
    "\n",
    "audio2face_service = Audio2FaceService()\n",
    "asr_service = ASRService()\n",
    "\n",
    "# writing the prompts in a file:\n",
    "file_asr_transcripts = \"./docs/prompts_transcripts_noisy.txt\"\n",
    "file_avatar_replies = \"./docs/full_model_avatar_replies_noisy.txt\"\n",
    "    \n",
    "with  open(file_asr_transcripts,\"a\", encoding=\"utf-8\")  as f1, open(file_avatar_replies,\"a\", encoding=\"utf-8\")  as f2:\n",
    "    while True:\n",
    "        print(\"Ask avatar\")\n",
    "        \n",
    "        # speech recognition\n",
    "        asr_service.run()\n",
    "        transcript = asr_service.transcript\n",
    "        print(\"ASR transcript:\",transcript)\n",
    "        \n",
    "        # inference block\n",
    "        text = extractor([(transcript,transcript, prompt(transcript), False)], data)[0][1]\n",
    "        # Detecting removing empty replies from the LLM\n",
    "        if len(text.strip()) == 0:\n",
    "            text = \"Sorry, I do not understand you!\"\n",
    "        \n",
    "        # removing half sequences in the end of LLM responses\n",
    "        output0 = remove_half_seq(text)\n",
    "\n",
    "        # tts \n",
    "        # modification for pronunciation:\n",
    "        output0 = phonetic_modification(output0) \n",
    "        \n",
    "        # saving the ASR transcripts and avatars answer\n",
    "        f1.write(transcript)\n",
    "        f1.write(\"\\n\")\n",
    "        f2.write(output0)\n",
    "        f2.write(\"\\n\")\n",
    "        \n",
    "        # tranforming the text into chunks to avoid sending long string to tts\n",
    "        output_ch_sk = splitting_text(output0)\n",
    "            \n",
    "        for text in output_ch_sk:\n",
    "            print(text)\n",
    "            audio = tts_service.synthesize(text, language_code=\"en-US\", sample_rate_hz=sample_rate_hz,  voice_name=\"English-US.Male-1\")\n",
    "            audio_bytes = audio.audio\n",
    "            audio2face_service.make_avatar_speaks(audio_bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f54a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
