{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2545daee-65d2-4360-8297-df889da19889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import IPython.display as ipd\n",
    "import grpc\n",
    "import os, sys, contextlib\n",
    "import riva.client\n",
    "from txtai.embeddings import Embeddings\n",
    "from txtai.pipeline import Extractor\n",
    "from ast import literal_eval\n",
    "from nemollm.api import NemoLLM\n",
    "from riva.client import RecognitionConfig, StreamingRecognitionConfig, AudioEncoding\n",
    "import riva.client.audio_io\n",
    "from riva.client.audio_io import MicrophoneStream\n",
    "from typing import Iterable\n",
    "import riva.client.proto.riva_asr_pb2 as rasr\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "915604af-2ead-48e4-a9f0-6e7254015e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "URI_TTS = os.get_env('URI_TTS')\n",
    "URI_ASR = os.get_env('URI_ASR')\n",
    "AVATAR_INSTANCE_PATH = os.get_env('AVATAR_INSTANCE_PATH')\n",
    "OPENAI_API_KEY = os.get_env('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d675b518-51e0-451c-b8d8-1a35f6cc3f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input audio devices:\n",
      "0: Microsoft Sound Mapper - Input\n",
      "1: Remote Audio\n",
      "4: Primary Sound Capture Driver\n",
      "5: Remote Audio\n",
      "9: Remote Audio\n"
     ]
    }
   ],
   "source": [
    "riva.client.audio_io.list_input_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ac9f08-0841-49df-b8cf-33fad79d4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your input device here\n",
    "ASR_INPUT_DEVICE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83516b40-7ddf-485b-b388-b5cb580c9e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio to Speech Module utilizing the Riva SDK\n",
    "config = riva.client.StreamingRecognitionConfig(\n",
    "    config=riva.client.RecognitionConfig(\n",
    "        encoding=riva.client.AudioEncoding.LINEAR_PCM,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=1,\n",
    "        profanity_filter=False,\n",
    "        enable_automatic_punctuation=True,\n",
    "        verbatim_transcripts=True,\n",
    "        sample_rate_hertz=16000\n",
    "    ),\n",
    "    interim_results=False,\n",
    ")\n",
    "\n",
    "\n",
    "class ASRService:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.auth = riva.client.Auth(uri=URI_ASR)\n",
    "        self.service = riva.client.ASRService(self.auth)\n",
    "        self.sample_rate_hz = 16000\n",
    "        self.file_streaming_chunk = 1600\n",
    "        self.transcript = \"\"\n",
    "        self.default_device_info = riva.client.audio_io.get_default_input_device_info()\n",
    "        self.default_device_index = None if self.default_device_info is None else self.default_device_info['index']\n",
    "\n",
    "    def run(self) -> None:\n",
    "        \"\"\"\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        print(\"ASR service running\")\n",
    "        with riva.client.audio_io.MicrophoneStream(\n",
    "                rate=self.sample_rate_hz,\n",
    "                chunk=self.file_streaming_chunk,\n",
    "                device=ASR_INPUT_DEVICE,\n",
    "        ) as audio_chunk_iterator:\n",
    "            #print(\"mic working\")\n",
    "            self.print_response(responses=self.service.streaming_response_generator(\n",
    "                audio_chunks=audio_chunk_iterator,\n",
    "                streaming_config=config))\n",
    "\n",
    "    def print_response(self, responses: Iterable[rasr.StreamingRecognizeResponse]) -> None:\n",
    "        \"\"\"\n",
    "        :param responses: Streaming Response\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.transcript = \"\"\n",
    "        for response in responses:\n",
    "            if not response.results:\n",
    "                continue\n",
    "\n",
    "            for result in response.results:\n",
    "                if not result.alternatives:\n",
    "                    continue\n",
    "                if result.is_final:\n",
    "                    partial_transcript = result.alternatives[0].transcript\n",
    "                    self.transcript += partial_transcript\n",
    "                    # print(self.transcript)\n",
    "                    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b45e6fa-b6e5-484a-bfb8-03826ec40451",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = URI_TTS\n",
    "auth = riva.client.Auth(uri=uri)\n",
    "\n",
    "tts_service = riva.client.SpeechSynthesisService(auth)\n",
    "\n",
    "sample_rate_hz = 44100\n",
    "req = { \n",
    "        \"language_code\"  : \"en-US\",\n",
    "        \"encoding\"       : riva.client.AudioEncoding.LINEAR_PCM ,   # Currently only LINEAR_PCM is supported\n",
    "        \"sample_rate_hz\" : sample_rate_hz,                          # Generate 44.1KHz audio\n",
    "        \"voice_name\"     : \"English-US.Female-1\"                    # The name of the voice to generate\n",
    "}\n",
    "nchannels = 1\n",
    "sampwidth = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e05d86b-29af-4de9-af11-80ce6f3c2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech to Audio2Face module utilizing the gRPC protocal from audio2face_streaming_utils\n",
    "from audio2face_streaming_utils import push_audio_track\n",
    "import riva.client\n",
    "import io\n",
    "from pydub import AudioSegment\n",
    "from scipy.io.wavfile import read\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Audio2FaceService:\n",
    "    def __init__(self, sample_rate=44100):\n",
    "        \"\"\"\n",
    "        :param sample_rate: sample rate\n",
    "        \"\"\"\n",
    "        self.a2f_url = 'localhost:50051'   # Set it to the port of your local host \n",
    "        self.sample_rate = 44100\n",
    "        self.avatar_instance = AVATAR_INSTANCE_PATH # Set it to the name of your Audio2Face Streaming Instance\n",
    "\n",
    "    def tts_to_wav(self, tts_byte, framerate=22050) -> str:\n",
    "        \"\"\"\n",
    "        :param tts_byte: tts data in byte\n",
    "        :param framerate: framerate\n",
    "        :return: wav byte\n",
    "        \"\"\"\n",
    "        seg = AudioSegment.from_raw(io.BytesIO(tts_byte), sample_width=2, frame_rate=22050, channels=1)\n",
    "        wavIO = io.BytesIO()\n",
    "        seg.export(wavIO, format=\"wav\")\n",
    "        rate, wav = read(io.BytesIO(wavIO.getvalue()))\n",
    "        return wav\n",
    "\n",
    "    def wav_to_numpy_float32(self, wav_byte) -> float:\n",
    "        \"\"\"\n",
    "        :param wav_byte: wav byte\n",
    "        :return: float32\n",
    "        \"\"\"\n",
    "        return wav_byte.astype(np.float32, order='C') / 32768.0\n",
    "\n",
    "    def get_tts_numpy_audio(self, audio) -> float:\n",
    "        \"\"\"\n",
    "        :param audio: audio from tts_to_wav\n",
    "        :return: float32 of the audio\n",
    "        \"\"\"\n",
    "        wav_byte = self.tts_to_wav(audio)\n",
    "        return self.wav_to_numpy_float32(wav_byte)\n",
    "\n",
    "    def make_avatar_speaks(self, audio) -> None:\n",
    "        \"\"\"\n",
    "        :param audio: tts audio\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        push_audio_track(self.a2f_url, self.get_tts_numpy_audio(audio), self.sample_rate, self.avatar_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1229a98e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Who are the leaders of the arena? ----\n",
      "\n",
      "The\n",
      " leaders\n",
      " of\n",
      " the\n",
      " arena\n",
      " may\n",
      " vary\n",
      " depending\n",
      " on\n",
      " the\n",
      " context\n",
      ".\n",
      " Could\n",
      " you\n",
      " please\n",
      " specify\n",
      " which\n",
      " arena\n",
      " you\n",
      " are\n",
      " referring\n",
      " to\n",
      "?\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def api(input):\n",
    "    system_prompt = f\"\"\"You are a helpful assistant.\"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo-1106\",\n",
    "      # model = \"gpt-4-1106-preview\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": input},\n",
    "      ],\n",
    "      stream=True\n",
    "    )\n",
    "\n",
    "    # response = completion.choices[0].message.content.strip()\n",
    "    return [completion]\n",
    "\n",
    "\n",
    "extractor = api\n",
    "\n",
    "#Ad-hoc questions\n",
    "question = \"Who are the leaders of the arena?\"\n",
    "\n",
    "#Ad-hoc questions\n",
    "question = \"Who are the leaders of the arena?\"\n",
    "\n",
    "print(\"----\", question, \"----\")\n",
    "completion = extractor(question)[0]\n",
    "for chunk in completion:        \n",
    "    output0 = chunk.choices[0].delta.content\n",
    "    print(output0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0fb5eaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function to filter out half sentences of the response of the LLM\n",
    "def remove_half_seq(text) -> str:\n",
    "    \"\"\"\n",
    "    :param text: a textual string\n",
    "    :return: a textual string similar or shorter than the input\n",
    "    \"\"\"\n",
    "    marks = [\".\", \"?\", \"!\",\":\",\",\",\";\"]\n",
    "    if text.strip()[-1] in marks:\n",
    "        return text\n",
    "    len_1= []\n",
    "    lens = []\n",
    "    for mark in marks:\n",
    "        splitted_text = text.strip().split(mark)\n",
    "        if len(splitted_text)>1:\n",
    "            len_mark= len(splitted_text[-1])\n",
    "            lens.append(len_mark)\n",
    "        elif len(splitted_text) == 1:\n",
    "            len_1.append(1)\n",
    "    if len(lens)>0:\n",
    "        return text.strip()[:(len(text)-min(lens))]\n",
    "    if len(len_1) == len(marks):\n",
    "        return text\n",
    "        \n",
    "    \n",
    "\n",
    "# function for tts modification for pronunciation:\n",
    "def phonetic_modification(text) -> str:\n",
    "    \"\"\"\n",
    "    :param text: a textual string\n",
    "    :return: a textual in which some words are tagged with new phonetics\n",
    "    \"\"\"\n",
    "    token_list = [\"Johanna\",\"Umeå\", \"AI\",\"LLMs\", \"II\", \"VII\", \"VI\", \"åre\" ]\n",
    "    modified_phonetic = [\"ˈjohana\",\"ˈoomijo\", \"ˈeiˈai\", \"ˈellˈellˈemz\", \"sekˈend\",\"ˈseven\", \"ˈsix\", \"\"]\n",
    "    for token, phon in zip(token_list,modified_phonetic) :\n",
    "        customized_phoneme = '<phoneme ph='+'\"'+ phon+ '\"'+'>'+token+'</phoneme>'\n",
    "        text = text.replace(token,customized_phoneme ) \n",
    "    return text\n",
    "\n",
    "def splitting_text(text) -> list:\n",
    "    \"\"\"\n",
    "    :param text: a textual string\n",
    "    :return:  list of <speak> tag added substrings of the input text\n",
    "    \"\"\"\n",
    "    ## splitting text considering .:\n",
    "    output_chunks0 = text.split(\".\")\n",
    "    # splitting text considering \\n:\n",
    "    output_chunks = []\n",
    "    for opt_ch in output_chunks0:\n",
    "        output_chunks.extend(opt_ch.split(\"\\n\"))\n",
    "    # adding <speak> tags to the substrings\n",
    "    output_ch_sk = []\n",
    "    for chunk in output_chunks:\n",
    "        if chunk.strip() ==\"\":\n",
    "            print(\"chunk\", chunk)\n",
    "        else:\n",
    "            output_ch_sk.append(f'<speak>{chunk}</speak>')\n",
    "    return output_ch_sk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "961fe080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask avatar\n",
      "ASR service running\n",
      "I just bought a computer mouse, but now I cannot connect it to my laptop. \n",
      "Done transcribing\n",
      "Sending audio data...\n",
      "SUCCESS\n",
      "Closed channel\n",
      "Sending audio data...\n",
      "SUCCESS\n",
      "Closed channel\n",
      "Sending audio data...\n",
      "SUCCESS\n",
      "Closed channel\n",
      "Sending audio data...\n",
      "SUCCESS\n",
      "Closed channel\n",
      "Sending audio data...\n",
      "SUCCESS\n",
      "Closed channel\n",
      "Sending audio data...\n",
      "SUCCESS\n",
      "Closed channel\n",
      "Ask avatar\n",
      "ASR service running\n",
      "Have to know if my mouth is turned on or not. \n",
      "Done transcribing\n",
      "Sending audio data...\n",
      "SUCCESS\n",
      "Closed channel\n",
      "Ask avatar\n",
      "ASR service running\n",
      "Have to know if my computer mouse is turned on or not. \n",
      "Done transcribing\n",
      "Sending audio data...\n",
      "SUCCESS\n",
      "Closed channel\n",
      "Sending audio data...\n",
      "SUCCESS\n",
      "Closed channel\n",
      "Sending audio data...\n",
      "SUCCESS\n",
      "Closed channel\n",
      "Ask avatar\n",
      "ASR service running\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsk avatar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# speech recognition\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m asr_service\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m     22\u001b[0m transcript \u001b[38;5;241m=\u001b[39m asr_service\u001b[38;5;241m.\u001b[39mtranscript\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(transcript)\n",
      "Cell \u001b[1;32mIn[5], line 39\u001b[0m, in \u001b[0;36mASRService.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASR service running\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m riva\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39maudio_io\u001b[38;5;241m.\u001b[39mMicrophoneStream(\n\u001b[0;32m     34\u001b[0m         rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_rate_hz,\n\u001b[0;32m     35\u001b[0m         chunk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_streaming_chunk,\n\u001b[0;32m     36\u001b[0m         device\u001b[38;5;241m=\u001b[39mASR_INPUT_DEVICE,\n\u001b[0;32m     37\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m audio_chunk_iterator:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m#print(\"mic working\")\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_response(responses\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mstreaming_response_generator(\n\u001b[0;32m     40\u001b[0m         audio_chunks\u001b[38;5;241m=\u001b[39maudio_chunk_iterator,\n\u001b[0;32m     41\u001b[0m         streaming_config\u001b[38;5;241m=\u001b[39mconfig))\n",
      "Cell \u001b[1;32mIn[5], line 49\u001b[0m, in \u001b[0;36mASRService.print_response\u001b[1;34m(self, responses)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m:param responses: Streaming Response\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m:return: None\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranscript \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m responses:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response\u001b[38;5;241m.\u001b[39mresults:\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\riva\\client\\asr.py:338\u001b[0m, in \u001b[0;36mASRService.streaming_response_generator\u001b[1;34m(self, audio_chunks, streaming_config)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03mGenerates speech recognition responses for fragments of speech audio in :param:`audio_chunks`.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;124;03mThe purpose of the method is to perform speech recognition \"online\" - as soon as\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03m    <https://docs.nvidia.com/deeplearning/riva/user-guide/docs/reference/protos/protos.html#riva-proto-riva-asr-proto>`_.\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    337\u001b[0m generator \u001b[38;5;241m=\u001b[39m streaming_request_generator(audio_chunks, streaming_config)\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstub\u001b[38;5;241m.\u001b[39mStreamingRecognize(generator, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth\u001b[38;5;241m.\u001b[39mget_auth_metadata()):\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\grpc\\_channel.py:541\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\grpc\\_channel.py:958\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_response_ready\u001b[39m():\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    954\u001b[0m         cygrpc\u001b[38;5;241m.\u001b[39mOperationType\u001b[38;5;241m.\u001b[39mreceive_message \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mdue\n\u001b[0;32m    955\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    956\u001b[0m     )\n\u001b[1;32m--> 958\u001b[0m _common\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcondition\u001b[38;5;241m.\u001b[39mwait, _response_ready)\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    960\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\grpc\\_common.py:156\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(wait_fn, wait_complete_fn, timeout, spin_cb)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wait_complete_fn():\n\u001b[1;32m--> 156\u001b[0m         _wait_once(wait_fn, MAXIMUM_WAIT_TIMEOUT, spin_cb)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m timeout\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\grpc\\_common.py:116\u001b[0m, in \u001b[0;36m_wait_once\u001b[1;34m(wait_fn, timeout, spin_cb)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait_once\u001b[39m(\n\u001b[0;32m    112\u001b[0m     wait_fn: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mbool\u001b[39m],\n\u001b[0;32m    113\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[0;32m    114\u001b[0m     spin_cb: Optional[Callable[[], \u001b[38;5;28;01mNone\u001b[39;00m]],\n\u001b[0;32m    115\u001b[0m ):\n\u001b[1;32m--> 116\u001b[0m     wait_fn(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spin_cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m         spin_cb()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Avatar + ASR block\n",
    "\n",
    "# K.B model PLUS ASR block + tts modification for pronunciation\n",
    "# Saving the ASR transcripts of the prompts\n",
    "# Filtering out half sequences in the LLM responses\n",
    "# resolving the issue of long input for tts block\n",
    "# saving avatars answer (users prompts are in prompts.txt)\n",
    "\n",
    "audio2face_service = Audio2FaceService()\n",
    "asr_service = ASRService()\n",
    "\n",
    "# writing the prompts in a file:\n",
    "file_asr_transcripts = \"./docs/prompts_transcripts.txt\"\n",
    "file_avatar_replies = \"./docs/full_model_avatar_replies.txt\"\n",
    "    \n",
    "with  open(file_asr_transcripts,\"a\")  as f1, open(file_avatar_replies,\"a\")  as f2:\n",
    "    while True:\n",
    "        print(\"Ask avatar\")\n",
    "        \n",
    "        # speech recognition\n",
    "        asr_service.run()\n",
    "        transcript = asr_service.transcript\n",
    "        print(transcript)\n",
    "        print(\"Done transcribing\")\n",
    "        \n",
    "        # inference block\n",
    "        completion = extractor(transcript)[0]\n",
    "        current_output = ''\n",
    "        for chunk in completion:\n",
    "            output0 = chunk.choices[0].delta.content\n",
    "            if output0 is None:\n",
    "                break\n",
    "            current_output += output0\n",
    "            if output0 in ['\\n', '.', '?', '!']:\n",
    "                # tts\n",
    "                # modification for pronunciation:\n",
    "                current_output = phonetic_modification(current_output)\n",
    "                current_output = f'<speak>{current_output}</speak>' \n",
    "\n",
    "                audio = tts_service.synthesize(current_output, language_code=\"en-US\", sample_rate_hz=sample_rate_hz,  voice_name=\"English-US.Male-1\")\n",
    "                audio_bytes = audio.audio\n",
    "                audio2face_service.make_avatar_speaks(audio_bytes)\n",
    "                current_output = ''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
